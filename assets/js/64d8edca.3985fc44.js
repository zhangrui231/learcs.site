"use strict";(self.webpackChunklearncs_set=self.webpackChunklearncs_set||[]).push([[7896],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},h="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),h=p(n),u=o,m=h["".concat(l,".").concat(u)]||h[u]||c[u]||i;return n?a.createElement(m,r(r({ref:t},d),{},{components:n})):a.createElement(m,r({ref:t},d))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[h]="string"==typeof e?e:o,r[1]=s;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},23268:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const i={title:"16-visualizing-data"},r="Visualizing data",s={unversionedId:"curriculum-resource/py4e/book/16",id:"curriculum-resource/py4e/book/16",title:"16-visualizing-data",description:"So far we have been learning the Python language and then learning",source:"@site/docs/curriculum-resource/py4e/book/16.md",sourceDirName:"curriculum-resource/py4e/book",slug:"/curriculum-resource/py4e/book/16",permalink:"/docs/curriculum-resource/py4e/book/16",draft:!1,tags:[],version:"current",frontMatter:{title:"16-visualizing-data"},sidebar:"tutorialSidebar",previous:{title:"15-database",permalink:"/docs/curriculum-resource/py4e/book/15"},next:{title:"01-intro",permalink:"/docs/curriculum-resource/py4e/book_zh/01"}},l={},p=[{value:"Building a OpenStreetMap from geocoded data",id:"building-a-openstreetmap-from-geocoded-data",level:2},{value:"Visualizing networks and interconnections",id:"visualizing-networks-and-interconnections",level:2},{value:"Visualizing mail data",id:"visualizing-mail-data",level:2}],d={toc:p},h="wrapper";function c(e){let{components:t,...i}=e;return(0,o.kt)(h,(0,a.Z)({},d,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"visualizing-data"},"Visualizing data"),(0,o.kt)("p",null,"So far we have been learning the Python language and then learning\nhow to use Python, the network, and databases to manipulate data."),(0,o.kt)("p",null,"In this chapter, we take a look at three complete applications that\nbring all of these things together to manage and visualize data. You\nmight use these applications as sample code to help get you started in\nsolving a real-world problem."),(0,o.kt)("p",null,"Each of the applications is a ZIP file that you can download and\nextract onto your computer and execute."),(0,o.kt)("h2",{id:"building-a-openstreetmap-from-geocoded-data"},"Building a OpenStreetMap from geocoded data"),(0,o.kt)("p",null,"In this project, we are using the OpenStreetMap geocoding API to\nclean up some user-entered geographic locations of university names and\nthen placing the data on an actual OpenStreetMap."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"An OpenStreetMap",src:n(5138).Z,width:"655",height:"395"}),"An OpenStreetMap"),(0,o.kt)("p",null,"To get started, download the application from:"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.py4e.com/code3/opengeo.zip"},"www.py4e.com/code3/opengeo.zip")),(0,o.kt)("p",null,"The first problem to solve is that these geocoding APIs are\nrate-limited to a certain number of requests per day. If you have a lot\nof data, you might need to stop and restart the lookup process several\ntimes. So we break the problem into two phases."),(0,o.kt)("p",null,"In the first phase we take our input \u201csurvey\u201d data in the file\n",(0,o.kt)("em",{parentName:"p"},"where.data")," and read it one line at a time, and retrieve the\ngeocoded information from Google and store it in a database\n",(0,o.kt)("em",{parentName:"p"},"geodata.sqlite"),". Before we use the geocoding API for each\nuser-entered location, we simply check to see if we already have the\ndata for that particular line of input. The database is functioning as a\nlocal \u201ccache\u201d of our geocoding data to make sure we never ask Google for\nthe same data twice."),(0,o.kt)("p",null,"You can restart the process at any time by removing the file\n",(0,o.kt)("em",{parentName:"p"},"geodata.sqlite"),"."),(0,o.kt)("p",null,"Run the ",(0,o.kt)("em",{parentName:"p"},"geoload.py")," program. This program will read the input\nlines in ",(0,o.kt)("em",{parentName:"p"},"where.data")," and for each line check to see if it is\nalready in the database. If we don\u2019t have the data for the location, it\nwill call the geocoding API to retrieve the data and store it in the\ndatabase."),(0,o.kt)("p",null,"Here is a sample run after there is already some data in the\ndatabase:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'Found in database AGH University of Science and Technology\n\nFound in database Academy of Fine Arts Warsaw Poland\n\nFound in database American University in Cairo\n\nFound in database Arizona State University\n\nFound in database Athens Information Technology\n\nRetrieving https://py4e-data.dr-chuck.net/\n   opengeo?q=BITS+Pilani\nRetrieved 794 characters {"type":"FeatureColl\n\nRetrieving https://py4e-data.dr-chuck.net/\n   opengeo?q=Babcock+University\nRetrieved 760 characters {"type":"FeatureColl\n\nRetrieving https://py4e-data.dr-chuck.net/\n   opengeo?q=Banaras+Hindu+University\nRetrieved 866 characters {"type":"FeatureColl\n...\n')),(0,o.kt)("p",null,"The first five locations are already in the database and so they are\nskipped. The program scans to the point where it finds new locations and\nstarts retrieving them."),(0,o.kt)("p",null,"The ",(0,o.kt)("em",{parentName:"p"},"geoload.py")," program can be stopped at any time, and there\nis a counter that you can use to limit the number of calls to the\ngeocoding API for each run. Given that the ",(0,o.kt)("em",{parentName:"p"},"where.data")," only has\na few hundred data items, you should not run into the daily rate limit,\nbut if you had more data it might take several runs over several days to\nget your database to have all of the geocoded data for your input."),(0,o.kt)("p",null,"Once you have some data loaded into ",(0,o.kt)("em",{parentName:"p"},"geodata.sqlite"),", you can\nvisualize the data using the ",(0,o.kt)("em",{parentName:"p"},"geodump.py")," program. This program\nreads the database and writes the file ",(0,o.kt)("em",{parentName:"p"},"where.js")," with the\nlocation, latitude, and longitude in the form of executable JavaScript\ncode."),(0,o.kt)("p",null,"A run of the ",(0,o.kt)("em",{parentName:"p"},"geodump.py")," program is as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"AGH University of Science and Technology, Czarnowiejska,\nCzarna Wie\u015b, Krowodrza, Krak\xf3w, Lesser Poland\nVoivodeship, 31-126, Poland 50.0657 19.91895\n\nAcademy of Fine Arts, Krakowskie Przedmie\u015bcie,\nNorthern \u015ar\xf3dmie\u015bcie, \u015ar\xf3dmie\u015bcie, Warsaw, Masovian\nVoivodeship, 00-046, Poland 52.239 21.0155\n...\n260 lines were written to where.js\nOpen the where.html file in a web browser to view the data.\n")),(0,o.kt)("p",null,"The file ",(0,o.kt)("em",{parentName:"p"},"where.html")," consists of HTML and JavaScript to\nvisualize a Google map. It reads the most recent data in\n",(0,o.kt)("em",{parentName:"p"},"where.js")," to get the data to be visualized. Here is the format\nof the ",(0,o.kt)("em",{parentName:"p"},"where.js")," file:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"myData = [\\\n[50.0657,19.91895,\\\n'AGH University of Science and Technology, Czarnowiejska,\\\nCzarna Wie\u015b, Krowodrza, Krak\xf3w, Lesser Poland\\\nVoivodeship, 31-126, Poland '],\\\n[52.239,21.0155,\\\n'Academy of Fine Arts, Krakowskie Przedmie\u015bciee,\\\n\u015ar\xf3dmie\u015bcie P\xf3\u0142nocne, \u015ar\xf3dmie\u015bcie, Warsaw,\\\nMasovian Voivodeship, 00-046, Poland'],\\\n   ...\\\n];\n")),(0,o.kt)("p",null,"This is a JavaScript variable that contains a list of lists. The\nsyntax for JavaScript list constants is very similar to Python, so the\nsyntax should be familiar to you."),(0,o.kt)("p",null,"Simply open ",(0,o.kt)("em",{parentName:"p"},"where.html")," in a browser to see the locations.\nYou can hover over each map pin to find the location that the geocoding\nAPI returned for the user-entered input. If you cannot see any data when\nyou open the ",(0,o.kt)("em",{parentName:"p"},"where.html")," file, you might want to check the\nJavaScript or developer console for your browser."),(0,o.kt)("h2",{id:"visualizing-networks-and-interconnections"},"Visualizing networks and interconnections"),(0,o.kt)("p",null,"In this application, we will perform some of the functions of a\nsearch engine. We will first spider a small subset of the web and run a\nsimplified version of the Google page rank algorithm to determine which\npages are most highly connected, and then visualize the page rank and\nconnectivity of our small corner of the web. We will use the D3\nJavaScript visualization library ",(0,o.kt)("a",{parentName:"p",href:"http://d3js.org/"},"http://d3js.org/")," to produce the visualization\noutput."),(0,o.kt)("p",null,"You can download and extract this application from:"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.py4e.com/code3/pagerank.zip"},"www.py4e.com/code3/pagerank.zip")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Page Ranking",src:n(3879).Z,width:"638",height:"496"}),"\nA Page Ranking"),(0,o.kt)("p",null,"The first program ( ",(0,o.kt)("em",{parentName:"p"},"spider.py"),") program crawls a web site and\npulls a series of pages into the database ( ",(0,o.kt)("em",{parentName:"p"},"spider.sqlite"),"),\nrecording the links between pages. You can restart the process at any\ntime by removing the ",(0,o.kt)("em",{parentName:"p"},"spider.sqlite")," file and rerunning\n",(0,o.kt)("em",{parentName:"p"},"spider.py"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Enter web url or enter: http://www.dr-chuck.com/\n['http://www.dr-chuck.com']\nHow many pages:2\n1 http://www.dr-chuck.com/ 12\n2 http://www.dr-chuck.com/csev-blog/ 57\nHow many pages:\n")),(0,o.kt)("p",null,"In this sample run, we told it to crawl a website and retrieve two\npages. If you restart the program and tell it to crawl more pages, it\nwill not re-crawl any pages already in the database. Upon restart it\ngoes to a random non-crawled page and starts there. So each successive\nrun of ",(0,o.kt)("em",{parentName:"p"},"spider.py")," is additive."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Enter web url or enter: http://www.dr-chuck.com/\n['http://www.dr-chuck.com']\nHow many pages:3\n3 http://www.dr-chuck.com/csev-blog 57\n4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1\n5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13\nHow many pages:\n")),(0,o.kt)("p",null,"You can have multiple starting points in the same database\u2014within the\nprogram, these are called \u201cwebs\u201d. The spider chooses randomly amongst\nall non-visited links across all the webs as the next page to\nspider."),(0,o.kt)("p",null,"If you want to dump the contents of the ",(0,o.kt)("em",{parentName:"p"},"spider.sqlite")," file,\nyou can run ",(0,o.kt)("em",{parentName:"p"},"spdump.py")," as follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"(5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')\n(3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')\n(1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')\n(1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')\n4 rows.\n")),(0,o.kt)("p",null,"This shows the number of incoming links, the old page rank, the new\npage rank, the id of the page, and the url of the page. The\n",(0,o.kt)("em",{parentName:"p"},"spdump.py")," program only shows pages that have at least one\nincoming link to them."),(0,o.kt)("p",null,"Once you have a few pages in the database, you can run page rank on\nthe pages using the ",(0,o.kt)("em",{parentName:"p"},"sprank.py")," program. You simply tell it how\nmany page rank iterations to run."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"How many iterations:2\n1 0.546848992536\n2 0.226714939664\n[(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]\n")),(0,o.kt)("p",null,"You can dump the database again to see that page rank has been\nupdated:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"(5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')\n(3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')\n(1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')\n(1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')\n4 rows.\n")),(0,o.kt)("p",null,"You can run ",(0,o.kt)("em",{parentName:"p"},"sprank.py")," as many times as you like and it will\nsimply refine the page rank each time you run it. You can even run\n",(0,o.kt)("em",{parentName:"p"},"sprank.py")," a few times and then go spider a few more pages with\n",(0,o.kt)("em",{parentName:"p"},"spider.py")," and then run ",(0,o.kt)("em",{parentName:"p"},"sprank.py")," to reconverge the\npage rank values. A search engine usually runs both the crawling and\nranking programs all the time."),(0,o.kt)("p",null,"If you want to restart the page rank calculations without respidering\nthe web pages, you can use ",(0,o.kt)("em",{parentName:"p"},"spreset.py")," and then restart\n",(0,o.kt)("em",{parentName:"p"},"sprank.py"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"How many iterations:50\n1 0.546848992536\n2 0.226714939664\n3 0.0659516187242\n4 0.0244199333\n5 0.0102096489546\n6 0.00610244329379\n...\n42 0.000109076928206\n43 9.91987599002e-05\n44 9.02151706798e-05\n45 8.20451504471e-05\n46 7.46150183837e-05\n47 6.7857770908e-05\n48 6.17124694224e-05\n49 5.61236959327e-05\n50 5.10410499467e-05\n[(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]\n")),(0,o.kt)("p",null,"For each iteration of the page rank algorithm it prints the average\nchange in page rank per page. The network initially is quite unbalanced\nand so the individual page rank values change wildly between iterations.\nBut in a few short iterations, the page rank converges. You should run\n",(0,o.kt)("em",{parentName:"p"},"sprank.py")," long enough that the page rank values converge."),(0,o.kt)("p",null,"If you want to visualize the current top pages in terms of page rank,\nrun ",(0,o.kt)("em",{parentName:"p"},"spjson.py")," to read the database and write the data for the\nmost highly linked pages in JSON format to be viewed in a web\nbrowser."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Creating JSON output on spider.json...\nHow many nodes? 30\nOpen force.html in a browser to view the visualization\n")),(0,o.kt)("p",null,"You can view this data by opening the file ",(0,o.kt)("em",{parentName:"p"},"force.html")," in\nyour web browser. This shows an automatic layout of the nodes and links.\nYou can click and drag any node and you can also double-click on a node\nto find the URL that is represented by the node."),(0,o.kt)("p",null,"If you rerun the other utilities, rerun ",(0,o.kt)("em",{parentName:"p"},"spjson.py")," and press\nrefresh in the browser to get the new data from\n",(0,o.kt)("em",{parentName:"p"},"spider.json"),"."),(0,o.kt)("h2",{id:"visualizing-mail-data"},"Visualizing mail data"),(0,o.kt)("p",null,"Up to this point in the book, you have become quite familiar with our\n",(0,o.kt)("em",{parentName:"p"},"mbox-short.txt")," and ",(0,o.kt)("em",{parentName:"p"},"mbox.txt")," data files. Now it is time\nto take our analysis of email data to the next level."),(0,o.kt)("p",null,"In the real world, sometimes you have to pull down mail data from\nservers. That might take quite some time and the data might be\ninconsistent, error-filled, and need a lot of cleanup or adjustment. In\nthis section, we work with an application that is the most complex so\nfar and pull down nearly a gigabyte of data and visualize it."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"A Word Cloud from the Sakai Developer List",src:n(76912).Z,width:"820",height:"877"}),"\nA Word Cloud from the Sakai Developer List"),(0,o.kt)("p",null,"You can download this application from:"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.py4e.com/code3/gmane.zip"},"https://www.py4e.com/code3/gmane.zip")),(0,o.kt)("p",null,"We will be using data from a free email list archiving service that\nwas called ",(0,o.kt)("em",{parentName:"p"},"gmane")," ","-"," the service has since been shut down and for\nthe purposes of this course, a partial archive has been kept at ",(0,o.kt)("a",{parentName:"p",href:"http://mbox.dr-chuck.net/"},"http://mbox.dr-chuck.net"),". The gmane\nservice was very popular with open source projects because it provided a\nnice searchable archive of their email activity."),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"http://mbox.dr-chuck.net/export.php"},"http://mbox.dr-chuck.net/export.php")),(0,o.kt)("p",null,"When the Sakai email data was spidered using this software, it\nproduced nearly a Gigabyte of data and took a number of runs on several\ndays. The file ",(0,o.kt)("em",{parentName:"p"},"README.txt")," in the above ZIP may have\ninstructions as to how you can download a pre-spidered copy of the\n",(0,o.kt)("em",{parentName:"p"},"content.sqlite")," file for a majority of the Sakai email corpus so\nyou don\u2019t have to spider for five days just to run the programs. If you\ndownload the pre-spidered content, you should still run the spidering\nprocess to catch up with more recent messages."),(0,o.kt)("p",null,"The first step is to spider the repository. The base URL is\nhard-coded in the ",(0,o.kt)("em",{parentName:"p"},"gmane.py")," and is hard-coded to the Sakai\ndeveloper list. You can spider another repository by changing that base\nurl. Make sure to delete the ",(0,o.kt)("em",{parentName:"p"},"content.sqlite")," file if you switch\nthe base url."),(0,o.kt)("p",null,"The ",(0,o.kt)("em",{parentName:"p"},"gmane.py")," file operates as a responsible caching spider\nin that it runs slowly and retrieves one mail message per second so as\nto avoid getting throttled. It stores all of its data in a database and\ncan be interrupted and restarted as often as needed. It may take many\nhours to pull all the data down. So you may need to restart several\ntimes."),(0,o.kt)("p",null,"Here is a run of ",(0,o.kt)("em",{parentName:"p"},"gmane.py")," retrieving the last five messages\nof the Sakai developer list:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"How many messages:10\nhttp://mbox.dr-chuck.net/sakai.devel/51410/51411 9460\n    nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...\\\nhttp://mbox.dr-chuck.net/sakai.devel/51411/51412 3379\\\n    samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...\\\nhttp://mbox.dr-chuck.net/sakai.devel/51412/51413 9903\\\n    da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...\\\nhttp://mbox.dr-chuck.net/sakai.devel/51413/51414 349265\\\n    m.shedid@elraed-it.com 2013-04-07 [building sakai] ...\\\nhttp://mbox.dr-chuck.net/sakai.devel/51414/51415 3481\\\n    samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...\\\nhttp://mbox.dr-chuck.net/sakai.devel/51415/51416 0\\\n\\\nDoes not start with From\\\n```\\\n\\\nThe program scans _content.sqlite_ from one up to the first\\\nmessage number not already spidered and starts spidering at that\\\nmessage. It continues spidering until it has spidered the desired number\\\nof messages or it reaches a page that does not appear to be a properly\\\nformatted message.\\\n\\\nSometimes the repository is missing a message. Perhaps administrators\\\ncan delete messages or perhaps they get lost. If your spider stops, and\\\nit seems it has hit a missing message, go into the SQLite Manager and\\\nadd a row with the missing id leaving all the other fields blank and\\\nrestart _gmane.py_. This will unstick the spidering process and\\\nallow it to continue. These empty messages will be ignored in the next\\\nphase of the process.\\\n\\\nOne nice thing is that once you have spidered all of the messages and\\\nhave them in _content.sqlite_, you can run _gmane.py_\\\nagain to get new messages as they are sent to the list.\\\n\\\nThe _content.sqlite_ data is pretty raw, with an inefficient\\\ndata model, and not compressed. This is intentional as it allows you to\\\nlook at _content.sqlite_ in the SQLite Manager to debug problems\\\nwith the spidering process. It would be a bad idea to run any queries\\\nagainst this database, as they would be quite slow.\\\n\\\nThe second process is to run the program _gmodel.py_. This\\\nprogram reads the raw data from _content.sqlite_ and produces a\\\ncleaned-up and well-modeled version of the data in the file\\\n_index.sqlite_. This file will be much smaller (often 10X\\\nsmaller) than _content.sqlite_ because it also compresses the\\\nheader and body text.\\\n\\\nEach time _gmodel.py_ runs it deletes and rebuilds\\\n_index.sqlite_, allowing you to adjust its parameters and edit\\\nthe mapping tables in _content.sqlite_ to tweak the data cleaning\\\nprocess. This is a sample run of _gmodel.py_. It prints a line\\\nout each time 250 mail messages are processed so you can see some\\\nprogress happening, as this program may run for a while processing\\\nnearly a Gigabyte of mail data.\\\n\\\n```\\\nLoaded allsenders 1588 and mapping 28 dns mapping 1\\\n1 2005-12-08T23:34:30-06:00 ggolden22@mac.com\\\n251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu\\\n501 2006-01-12T11:17:34-05:00 lance@indiana.edu\\\n751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu\\\n...\\\n```\\\n\\\nThe _gmodel.py_ program handles a number of data cleaning\\\ntasks.\\\n\\\nDomain names are truncated to two levels for .com, .org, .edu, and\\\n.net. Other domain names are truncated to three levels. So si.umich.edu\\\nbecomes umich.edu and caret.cam.ac.uk becomes cam.ac.uk. Email addresses\\\nare also forced to lower case, and some of the @gmane.org address like the following\\\n\\\n```\\\narwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org\\\n```\\\n\\\nare converted to the real address whenever there is a matching real\\\nemail address elsewhere in the message corpus.\\\n\\\nIn the _mapping.sqlite_ database there are two tables that\\\nallow you to map both domain names and individual email addresses that\\\nchange over the lifetime of the email list. For example, Steve Githens\\\nused the following email addresses as he changed jobs over the life of\\\nthe Sakai developer list:\\\n\\\n```\\\ns-githens@northwestern.edu\\\nsgithens@cam.ac.uk\\\nswgithen@mtu.edu\\\n```\\\n\\\nWe can add two entries to the Mapping table in\\\n_mapping.sqlite_ so _gmodel.py_ will map all three to one\\\naddress:\\\n\\\n```\\\ns-githens@northwestern.edu ->  swgithen@mtu.edu\\\nsgithens@cam.ac.uk -> swgithen@mtu.edu\\\n```\\\n\\\nYou can also make similar entries in the DNSMapping table if there\\\nare multiple DNS names you want mapped to a single DNS. The following\\\nmapping was added to the Sakai data:\\\n\\\n```\\\niupui.edu -> indiana.edu\\\n```\\\n\\\nso all the accounts from the various Indiana University campuses are\\\ntracked together.\\\n\\\nYou can rerun the _gmodel.py_ over and over as you look at the\\\ndata, and add mappings to make the data cleaner and cleaner. When you\\\nare done, you will have a nicely indexed version of the email in\\\n_index.sqlite_. This is the file to use to do data analysis. With\\\nthis file, data analysis will be really quick.\\\n\\\nThe first, simplest data analysis is to determine \u201cwho sent the most\\\nmail?\u201d and \u201cwhich organization sent the most mail\u201d? This is done using\\\n_gbasic.py_:\\\n\\\n```\\\nHow many to dump? 5\\\nLoaded messages= 51330 subjects= 25033 senders= 1584\\\n\\\nTop 5 Email list participants\\\nsteve.swinsburg@gmail.com 2657\\\nazeckoski@unicon.net 1742\\\nieb@tfd.co.uk 1591\\\ncsev@umich.edu 1304\\\ndavid.horwitz@uct.ac.za 1184\\\n\\\nTop 5 Email list organizations\\\ngmail.com 7339\\\numich.edu 6243\\\nuct.ac.za 2451\\\nindiana.edu 2258\\\nunicon.net 2055\\\n```\\\n\\\nNote how much more quickly _gbasic.py_ runs compared to\\\n_gmane.py_ or even _gmodel.py_. They are all working on\\\nthe same data, but _gbasic.py_ is using the compressed and\\\nnormalized data in _index.sqlite_. If you have a lot of data to\\\nmanage, a multistep process like the one in this application may take a\\\nlittle longer to develop, but will save you a lot of time when you\\\nreally start to explore and visualize your data.\\\n\\\nYou can produce a simple visualization of the word frequency in the\\\nsubject lines in the file _gword.py_:\\\n\\\n```\\\nRange of counts: 33229 129\\\nOutput written to gword.js\\\n```\\\n\\\nThis produces the file _gword.js_ which you can visualize\\\nusing _gword.htm_ to produce a word cloud similar to the one at\\\nthe beginning of this section.\\\n\\\nA second visualization is produced by _gline.py_. It computes\\\nemail participation by organizations over time.\\\n\\\n```\\\nLoaded messages= 51330 senders= 1584\\\nTop 10 Oranizations\\\n['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',\\\n'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',\\\n'stanford.edu', 'ox.ac.uk']\\\nOutput written to gline.js\\\n```\\\n\\\nIts output is written to _gline.js_ which is visualized using\\\n_gline.htm_.\\\n\\\n![Sakai Mail Activity by Organization](/img/py4e/mailorg.png)Sakai Mail Activity by\\\nOrganization\\\n\\\nThis is a relatively complex and sophisticated application and has\\\nfeatures to do some real data retrieval, cleaning, and\\\nvisualization.\\\n\\\n* * *\\\n\\\nIf you find a mistake in this book, feel free to send me a fix using\\\n[Github](https://github.com/csev/py4e/tree/master/book3).\\\n\\\n")))}c.isMDXComponent=!0},5138:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/openstreet-map-bdb1cf047e38b81cce53165936d80a72.png"},3879:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/pagerank-43a86eee6afdeb9e1dae77201da132dc.png"},76912:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/wordcloud-429c3ddae60f2ceedbdfe9633155bc82.png"}}]);