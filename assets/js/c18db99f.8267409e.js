"use strict";(self.webpackChunklearncs_set=self.webpackChunklearncs_set||[]).push([[6204],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var o=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(n),m=a,h=d["".concat(s,".").concat(m)]||d[m]||c[m]||r;return n?o.createElement(h,i(i({ref:t},u),{},{components:n})):o.createElement(h,i({ref:t},u))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,i[1]=l;for(var p=2;p<r;p++)i[p]=n[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},47082:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var o=n(87462),a=(n(67294),n(3905));const r={title:"Lab11"},i="Lab 11",l={unversionedId:"curriculum-resource/cs61c/labs/lab11",id:"curriculum-resource/cs61c/labs/lab11",title:"Lab11",description:"Objectives:",source:"@site/docs/curriculum-resource/cs61c/labs/lab11.md",sourceDirName:"curriculum-resource/cs61c/labs",slug:"/curriculum-resource/cs61c/labs/lab11",permalink:"/docs/curriculum-resource/cs61c/labs/lab11",draft:!1,tags:[],version:"current",frontMatter:{title:"Lab11"},sidebar:"tutorialSidebar",previous:{title:"Lab10",permalink:"/docs/curriculum-resource/cs61c/labs/lab10"},next:{title:"Project1",permalink:"/docs/curriculum-resource/cs61c/projects/proj1"}},s={},p=[{value:"Objectives:",id:"objectives",level:2},{value:"Setup",id:"setup",level:2},{value:"Background Information",id:"background-information",level:2},{value:"Avoid Global Variables",id:"avoid-global-variables",level:3},{value:"Documentation, and Additional Resources",id:"documentation-and-additional-resources",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 0: Generating an Input File for Spark",id:"exercise-0-generating-an-input-file-for-spark",level:3},{value:"Exercise 1: Running Word Count",id:"exercise-1-running-word-count",level:3},{value:"Exercise 2: How many documents does each word appear in?",id:"exercise-2-how-many-documents-does-each-word-appear-in",level:3},{value:"Check-off",id:"check-off",level:4},{value:"Exercise 3: Full Text Index Creation",id:"exercise-3-full-text-index-creation",level:3},{value:"Check-off",id:"check-off-1",level:4},{value:"Exercise 4: What\u2019s the most popular word?",id:"exercise-4-whats-the-most-popular-word",level:3},{value:"Check-off",id:"check-off-2",level:3},{value:"Exercise 2:",id:"exercise-2",level:4},{value:"Exercise 3:",id:"exercise-3",level:4},{value:"Exercise 4:",id:"exercise-4",level:4}],u={toc:p},d="wrapper";function c(e){let{components:t,...n}=e;return(0,a.kt)(d,(0,o.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"lab-11"},"Lab 11"),(0,a.kt)("h2",{id:"objectives"},"Objectives:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Get hands-on experience running MapReduce and gain a deeper understanding of the MapReduce paradigm."),(0,a.kt)("li",{parentName:"ul"},"Become more familiar with Apache Spark and get hands on experience with running Spark on a local installation."),(0,a.kt)("li",{parentName:"ul"},"Learn how to apply the MapReduce paradigm to Spark by implementing certain problems/algorithms in Spark.")),(0,a.kt)("h2",{id:"setup"},"Setup"),(0,a.kt)("p",null,"Pull the lab files from the lab starter repository with"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ git pull starter master\n\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Be aware, this lab is only going to work on Hive machines.")),(0,a.kt)("p",null,"You will also be working with Spark (in Python!), so you may need to brush up a bit on your Python!!!! To be able to run Spark, you must create a virtual environment using the correct version of Python. This can be done as such:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ conda create --name lab11env python=2.7\n\n")),(0,a.kt)("p",null,"Respond to the prompt to install packages with \u201cy\u201d (with no quotes). You can (and should) ignore any warnings about conda being out of date. These will take about 30 seconds to install. Finally, run the following command to activate the virtual environment:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ source activate lab11env\n\n")),(0,a.kt)("p",null,"This will put you in a virtual environment needed for this lab. ",(0,a.kt)("strong",{parentName:"p"},"Please remember that if you exit the virtual environment and want to return to work on the lab, you must re-run ",(0,a.kt)("inlineCode",{parentName:"strong"},"source activate lab11env")," first for Spark to work.")),(0,a.kt)("p",null,"In addition, when logged into the Hive machines, run the following command in your Terminal:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"If you do not do this, Spark will throw an error about being unable to find the correct Java version.")),(0,a.kt)("h2",{id:"background-information"},"Background Information"),(0,a.kt)("p",null,"In lecture we\u2019ve exposed you to cluster computing (in particular, the MapReduce framework), how it is set up and executed, but now it\u2019s time to get some hands-on experience running programs with a cluster computing framework!"),(0,a.kt)("p",null,"In this lab, we will be introducing you to a cluster computing framework called Spark. Spark was developed right at Berkeley before being donated to the Apache Software Foundation in 2013. We will be writing Python code to run in Spark to give us some practice in writing Map and Reduce routines."),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://spark.apache.org/"},"Spark")," has its own website, so you are free to try to install it onto your local machines, although it may be easier to ssh into the lab computers to complete this lab."),(0,a.kt)("h3",{id:"avoid-global-variables"},"Avoid Global Variables"),(0,a.kt)("p",null,"When using Spark, avoid using global variables! This defeats the purpose of having multiple tasks running in parallel and creates a bottleneck when multiple tasks try to access the same global variable. As a result, most algorithms will be implemented without the use of global variables."),(0,a.kt)("h3",{id:"documentation-and-additional-resources"},"Documentation, and Additional Resources"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A quickstart programming guide for Spark (click the Python tab to see the Python code) is ",(0,a.kt)("a",{parentName:"li",href:"https://spark.apache.org/docs/latest/rdd-programming-guide.html"},"available here"),"!"),(0,a.kt)("li",{parentName:"ul"},"The version of Spark we will be using will be 1.1.0 and the link to the API documentation is ",(0,a.kt)("a",{parentName:"li",href:"https://spark.apache.org/docs/latest/api/python/index.html"},"available here")," (Note that the docs likely say a different version, but the API should be compatible).")),(0,a.kt)("h2",{id:"exercises"},"Exercises"),(0,a.kt)("p",null,"Note: Different exercises may be solvable or needed to be solved by reconsidering how map(), flat","_","map() and reduce() are implemented and called and in which order, so keep this in mind when calling whichever you must use."),(0,a.kt)("p",null,"The following exercises use sample input files, which can be found in ",(0,a.kt)("inlineCode",{parentName:"p"},"seqFiles/"),"."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"billOfRights.txt.seq")," \u2013 the first 10 Amendments of the US constitution split into separate documents (a very small input)"),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"complete-works-mark-twain.txt.seq")," \u2013 The Complete Works of Mark Twain (a medium-sized input)")),(0,a.kt)("p",null,"Notice the ",(0,a.kt)("inlineCode",{parentName:"p"},".seq")," extension, which signifies a sequence file that is readable by Spark. These are NOT human-readable. Spark supports other input formats, but you will not need to worry about that for this lab."),(0,a.kt)("p",null,"The human-readable text file version of each is included in ",(0,a.kt)("inlineCode",{parentName:"p"},"textFiles/")," so you can open those to get a sense of the contents of each file."),(0,a.kt)("p",null,"Although an exercise may not explicitly ask you to use it, we recommend testing your code on the ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights")," data set first in order to verify correct behavior and help you debug."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Reminder:")," this lab will only work on Hive machines and requires you to first activate a python virtual environment, as described above."),(0,a.kt)("h3",{id:"exercise-0-generating-an-input-file-for-spark"},"Exercise 0: Generating an Input File for Spark"),(0,a.kt)("p",null,"In this lab, we\u2019ll be working heavily with textual data. We have some pre-generated datasets as indicated above, but it\u2019s always more fun to use a dataset that you find interesting. This section of the lab will walk you through generating your own dataset using works from Project Gutenberg (a database of public-domain literary works)."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Step 1:")," Head over to ",(0,a.kt)("a",{parentName:"p",href:"https://www.gutenberg.org/"},"Project Gutenberg"),", pick a work of your choosing, and download the \u201cPlain Text UTF\u20138\u201d version into your lab directory."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Step 2:")," Open up the file you downloaded in your favorite text editor and insert ",(0,a.kt)("inlineCode",{parentName:"p"},"\u2014END.OF.DOCUMENT\u2014")," by itself on a new line wherever you want Spark to split the input file into separate ",(0,a.kt)("inlineCode",{parentName:"p"},"(key, value)")," pairs. The importer we\u2019re using will assign an arbitrary key (like ",(0,a.kt)("inlineCode",{parentName:"p"},"doc_xyz"),") and the value will be the contents of our input file between two ",(0,a.kt)("inlineCode",{parentName:"p"},"\u2014END.OF.DOCUMENT\u2014")," markers. You\u2019ll want to break the work into reasonably-sized chunks, but don\u2019t spend too much time on this part (chapters/sections within a single work or individual works in a body of works are good splitting points)."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Step 3:")," Now, we\u2019re going to run our Importer to generate a ",(0,a.kt)("inlineCode",{parentName:"p"},".seq")," file that we can pass into the Spark programs we\u2019ll write. The importer is actually a MapReduce program, written using Hadoop! You can take a look at ",(0,a.kt)("inlineCode",{parentName:"p"},"Importer.java")," if you want, but the implementation details aren\u2019t important for this part of the lab. You can generate your input file like so:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ make generate-input myinput=YOUR_FILE_FROM_STEP_2.txt\n\n")),(0,a.kt)("p",null,"Your generated ",(0,a.kt)("inlineCode",{parentName:"p"},".seq")," file can now be found in the ",(0,a.kt)("inlineCode",{parentName:"p"},"seqFiles/")," directory in your lab directory. When you complete other exercise in this lab, run them on your downloaded file as well and investigate the results."),(0,a.kt)("h3",{id:"exercise-1-running-word-count"},"Exercise 1: Running Word Count"),(0,a.kt)("p",null,"For this exercise you will use the already-completed ",(0,a.kt)("inlineCode",{parentName:"p"},"wordCount.py"),". Open the file and take a look. Make sure you understand what the file is attempting to do."),(0,a.kt)("p",null,"You can run it on the ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights")," text with the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ spark-submit wordCount.py seqFiles/billOfRights.txt.seq\n\n")),(0,a.kt)("p",null,"Where ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-submit")," takes a python file describing a series of map and reduce steps, distributes that files between different worker processors (often across many physical computers, but just across local processors for this lab), and provides the ",(0,a.kt)("inlineCode",{parentName:"p"},".seq")," file as an input to that python file."),(0,a.kt)("p",null,"In this case, the command will run ",(0,a.kt)("inlineCode",{parentName:"p"},"wordCount.py")," over ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights.txt.seq"),". Your output should be visible in ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-wc-out-wordCount/part-00000"),"."),(0,a.kt)("p",null,"Next, try your the code on the larger input file ",(0,a.kt)("inlineCode",{parentName:"p"},"complete-works-mark-twain.txt.seq"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ spark-submit wordCount.py seqFiles/complete-works-mark-twain.txt.seq\n\n")),(0,a.kt)("p",null,"Your output for this command will be located in the same ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-wc-out-wordCount/part-00000")," file (overwriting the previous results). Search through the file for a word like ",(0,a.kt)("inlineCode",{parentName:"p"},"'the'")," to get a better understanding of the output."),(0,a.kt)("h3",{id:"exercise-2-how-many-documents-does-each-word-appear-in"},"Exercise 2: How many documents does each word appear in?"),(0,a.kt)("p",null,"Earlier, we used the ",(0,a.kt)("inlineCode",{parentName:"p"},"\u2014END.OF.DOCUMENT\u2014")," token to split a text file into multiple documents. The sample files included in this lab are also split into documents. For example, ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights.txt")," is split into 10 documents (one for each amendment). For this exercise we want to count how many documents each word appears in. For example, ",(0,a.kt)("inlineCode",{parentName:"p"},'"Amendment"')," should appear in all 10 documents of ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights.txt"),"."),(0,a.kt)("p",null,"Open ",(0,a.kt)("inlineCode",{parentName:"p"},"perWordDocumentCount.py"),". It currently contains code that will execute the same functionality as ",(0,a.kt)("inlineCode",{parentName:"p"},"wordCount.py"),". Modify it to count the number of documents containing each word rather than the number of times each word occurs in the input and to sort that output in alphabetical order."),(0,a.kt)("p",null,"To help you with understanding the code, we have added some comments, but you will also need to take a look at the list of Spark ",(0,a.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"},"transformations")," for a more detailed explanation of the methods that can be used in Spark. There are methods that you can use to help sort an output or remove duplicate items. To help with distinguishing when a word appears in a document, you will want to make use of the document ID as well \u2013 this is mentioned in the comments of ",(0,a.kt)("inlineCode",{parentName:"p"},"flatMapFunc"),". Just because we gave you an outline doesn\u2019t mean you need to stick to it, feel free to add/remove transformations as you see fit. You\u2019re also encouraged to rename functions to more useful titles."),(0,a.kt)("p",null,"You can test ",(0,a.kt)("inlineCode",{parentName:"p"},"perWordDocumentCount.py")," (with results in ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-wc-out-perWordDocumentCount/part-00000"),") with the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ spark-submit perWordDocumentCount.py seqFiles/billOfRights.txt.seq\n\n")),(0,a.kt)("p",null,"You should also try it on the other sequence files you have to look for some interesting results."),(0,a.kt)("h4",{id:"check-off"},"Check-off"),(0,a.kt)("p",null,"Explain your modifications to ",(0,a.kt)("inlineCode",{parentName:"p"},"perWordDocumentCount.py")," to your TA."),(0,a.kt)("p",null,"Show your output from ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights")," to the TA. In particular, what values did you get for ",(0,a.kt)("inlineCode",{parentName:"p"},'"Amendment"'),", ",(0,a.kt)("inlineCode",{parentName:"p"},'"the"'),", and ",(0,a.kt)("inlineCode",{parentName:"p"},'"arms"'),"? Do these values make sense? You can confirm your results by looking through the ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights.txt")," file."),(0,a.kt)("h3",{id:"exercise-3-full-text-index-creation"},"Exercise 3: Full Text Index Creation"),(0,a.kt)("p",null,"Next, for each word and document in which that word appears at least once, we want to generate a list of index into the document for EACH appearance of the word, where an index is defined as the number of words since the beginning of the document (with the first word being index 0). Also make sure the output is sorted alphabetically by the word. Your output should have lines that look like the following (minor line formatting details don\u2019t matter):"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"(word1  document1-id, word# word# ...)\n(word1  document2-id, word# word# ...)\n. . .\n(word2  document1-id, word# word# ...)\n(word2  document3-id, word# word# ...)\n. . .\n\n")),(0,a.kt)("p",null,"Notice that there will be a line of output for EACH document in which that word appears and EACH word and document pair should only have ONE list of indices. Remember that you need to also keep track of the document ID as well."),(0,a.kt)("p",null,"For example, given a document with the text ",(0,a.kt)("inlineCode",{parentName:"p"},"With great power comes great responsibility"),", the word ",(0,a.kt)("inlineCode",{parentName:"p"},"With")," appears at index 0 while the word ",(0,a.kt)("inlineCode",{parentName:"p"},"great")," appears at index 1 and 4, and the output would look like:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"('comes doc_somerandomnumbers', 3)\n('great doc_somerandomnumbers', 1 4)\n('power doc_somerandomnumbers', 2)\n('responsibility doc_somerandomnumbers', 5)\n('With doc_somerandomnumbers', 0)\n\n")),(0,a.kt)("p",null,"The file you should edit to do this task is ",(0,a.kt)("inlineCode",{parentName:"p"},"createIndices.py"),". For this exercise, you may not need all the functions we have provided. If a function is not used, feel free to remove the method that is trying to call it. Make sure your output for this is sorted as well (just like in the previous exercise)."),(0,a.kt)("p",null,"You can test by running the script with ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-submit"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ spark-submit createIndices.py seqFiles/billOfRights.txt.seq\n\n")),(0,a.kt)("p",null,"The results are stored in ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-wc-out-createIndices/part-00000"),". The output from running this will be a large file. In order to more easily look at its contents, you can use the commands ",(0,a.kt)("inlineCode",{parentName:"p"},"cat"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"head"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"more"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"grep"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ head -25 OUTPUTFILE       # view the first 25 lines of output\n$ cat OUTPUTFILE | more     # scroll through output one screen at a time (use Space)\n$ cat OUTPUTFILE | grep the # output only lines containing 'the' (case-sensitive)\n\n")),(0,a.kt)("p",null,"Make sure to verify your output. Open ",(0,a.kt)("inlineCode",{parentName:"p"},"billOfRights.txt")," and pick a few words. Manually count a few of their word indices and make sure they all appear in your output file."),(0,a.kt)("h4",{id:"check-off-1"},"Check-off"),(0,a.kt)("p",null,"Explain your code in ",(0,a.kt)("inlineCode",{parentName:"p"},"createIndices.py")," to your TA. Next, run:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ spark-submit createIndices.py seqFiles/complete-works-mark-twain.txt.seq\n\n")),(0,a.kt)("p",null,"Show your TA the first page of your output for the word \u201cMark\u201d in ",(0,a.kt)("inlineCode",{parentName:"p"},"complete-works-mark-twain.txt.seq")," to verify correct output. You can do this by running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ cat spark-wc-out-createIndices/part-00000 | grep Mark | less\n\n")),(0,a.kt)("h3",{id:"exercise-4-whats-the-most-popular-word"},"Exercise 4: What\u2019s the most popular word?"),(0,a.kt)("p",null,"Use Spark to determine what the most popular word is in the Bill of Rights by generating a list of ",(0,a.kt)("inlineCode",{parentName:"p"},"(count, word)")," tuples in decsending order. We have copied over the code from ",(0,a.kt)("inlineCode",{parentName:"p"},"wordCount.py")," into a new script ",(0,a.kt)("inlineCode",{parentName:"p"},"mostPopular.py")," since it is a good starting point."),(0,a.kt)("p",null,(0,a.kt)("em",{parentName:"p"},"Hint:")," After the ",(0,a.kt)("inlineCode",{parentName:"p"},"reduceByKey")," operation has been run, you can still apply additional map operations to the data. Looking at the arguments for ",(0,a.kt)("inlineCode",{parentName:"p"},"sortByKey")," may save you a lot of scrolling as well."),(0,a.kt)("p",null,"To test your code, run:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"$ spark-submit mostPopular.py seqFiles/billOfRights.txt.seq\n\n")),(0,a.kt)("p",null,"The results are stored in ",(0,a.kt)("inlineCode",{parentName:"p"},"spark-wc-out-mostPopular/part-00000"),". As a fun exercise, try doing this on the book you downloaded in Exercise 0!"),(0,a.kt)("h3",{id:"check-off-2"},"Check-off"),(0,a.kt)("p",null,"Run ",(0,a.kt)("inlineCode",{parentName:"p"},"./submit")," in the ",(0,a.kt)("inlineCode",{parentName:"p"},"lab11")," folder, then submit your code to the ",(0,a.kt)("strong",{parentName:"p"},"Lab Autograder")," assignment. ",(0,a.kt)("strong",{parentName:"p"},"Remember to commit the ",(0,a.kt)("inlineCode",{parentName:"strong"},"outputs")," folder!"),". Below are some questions to consider:"),(0,a.kt)("h4",{id:"exercise-2"},"Exercise 2:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Explain your modifications to ",(0,a.kt)("inlineCode",{parentName:"li"},"perWordDocumentCount.py"))),(0,a.kt)("h4",{id:"exercise-3"},"Exercise 3:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Explain your modifications to ",(0,a.kt)("inlineCode",{parentName:"li"},"createIndices.py"))),(0,a.kt)("h4",{id:"exercise-4"},"Exercise 4:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Explain your code to the TA")))}c.isMDXComponent=!0}}]);