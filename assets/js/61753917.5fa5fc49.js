"use strict";(self.webpackChunklearncs_set=self.webpackChunklearncs_set||[]).push([[3135],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},d=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(n),h=a,m=c["".concat(s,".").concat(h)]||c[h]||u[h]||i;return n?r.createElement(m,o(o({ref:t},d),{},{components:n})):r.createElement(m,o({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:a,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},85863:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var r=n(87462),a=(n(67294),n(3905));const i={title:"Lab10"},o="Lab 10",l={unversionedId:"curriculum-resource/cs61c/labs/lab10",id:"curriculum-resource/cs61c/labs/lab10",title:"Lab10",description:"Objectives:",source:"@site/docs/curriculum-resource/cs61c/labs/lab10.md",sourceDirName:"curriculum-resource/cs61c/labs",slug:"/curriculum-resource/cs61c/labs/lab10",permalink:"/docs/curriculum-resource/cs61c/labs/lab10",draft:!1,tags:[],version:"current",frontMatter:{title:"Lab10"},sidebar:"tutorialSidebar",previous:{title:"Lab09",permalink:"/docs/curriculum-resource/cs61c/labs/lab09"},next:{title:"Lab11",permalink:"/docs/curriculum-resource/cs61c/labs/lab11"}},s={},p=[{value:"Objectives:",id:"objectives",level:2},{value:"Setup",id:"setup",level:2},{value:"Part 1: Multi-threading programming using OpenMP",id:"part-1-multi-threading-programming-using-openmp",level:2},{value:"Exercise 1 - OpenMP Hello World",id:"exercise-1---openmp-hello-world",level:3},{value:"Exercise 2 - Vector Addition",id:"exercise-2---vector-addition",level:2},{value:"Exercise 3 - Dot Product",id:"exercise-3---dot-product",level:2},{value:"Part 2: Intro to multi-processing programming",id:"part-2-intro-to-multi-processing-programming",level:2},{value:"Background - Http Web Server and Multi-processing",id:"background---http-web-server-and-multi-processing",level:3},{value:"Notes",id:"notes",level:4},{value:"Exercise:",id:"exercise",level:3},{value:"Speedup Requirements",id:"speedup-requirements",level:2},{value:"Checkoff",id:"checkoff",level:2},{value:"Part1",id:"part1",level:3},{value:"Exercise 2:",id:"exercise-2",level:4},{value:"Exercise 3:",id:"exercise-3",level:4},{value:"Part2",id:"part2",level:3}],d={toc:p},c="wrapper";function u(e){let{components:t,...i}=e;return(0,a.kt)(c,(0,r.Z)({},d,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"lab-10"},"Lab 10"),(0,a.kt)("h2",{id:"objectives"},"Objectives:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Learn about basic OpenMP directives."),(0,a.kt)("li",{parentName:"ul"},"Write code to learn two ways of how ",(0,a.kt)("inlineCode",{parentName:"li"},"#pragma omp for")," could be implemented. Learn about false sharing."),(0,a.kt)("li",{parentName:"ul"},"Larn about basic multi-processing programming.")),(0,a.kt)("h2",{id:"setup"},"Setup"),(0,a.kt)("p",null,"Pull the Lab 10 files from the lab starter repository with"),(0,a.kt)("h2",{id:"part-1-multi-threading-programming-using-openmp"},"Part 1: Multi-threading programming using OpenMP"),(0,a.kt)("p",null,"OpenMP stands for Open specification for Multi-Processing. It is a framework that offers a C interface. It is not a built-in part of the language \u2013 most OpenMP features are directives to the compiler."),(0,a.kt)("p",null,"Benefits of multi-threaded programming using OpenMP include:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Very simple interface allows a programmer to separate a program into serial regions and parallel regions."),(0,a.kt)("li",{parentName:"ul"},"Convenient synchronization control (data race bugs in POSIX threads are very hard to trace).")),(0,a.kt)("p",null,"In this lab, we will practice some basic usage of OpenMP. ",(0,a.kt)("strong",{parentName:"p"},"Feel free to build and use OpenMP on your own machine, but it would be the easiest to work on hive machines as they have OpenMP built and ready for use.")),(0,a.kt)("h3",{id:"exercise-1---openmp-hello-world"},"Exercise 1 - OpenMP Hello World"),(0,a.kt)("p",null,"Consider the implementation of Hello World (",(0,a.kt)("inlineCode",{parentName:"p"},"hello.c"),"):"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'int main() {\n    #pragma omp parallel\n    {\n        int thread_ID = omp_get_thread_num();\n        printf(" hello world %d\\n", thread_ID);\n    }\n}\n\n')),(0,a.kt)("p",null,"This program will fork off the default number of threads and each thread will print out \u201chello world\u201d in addition to which thread number it is. You can change the number of OpenMP threads by setting the environment variable ",(0,a.kt)("inlineCode",{parentName:"p"},"OMP_NUM_THREADS")," or by using the ",(0,a.kt)("a",{parentName:"p",href:"https://gcc.gnu.org/onlinedocs/libgomp/omp_005fset_005fnum_005fthreads.html"},(0,a.kt)("inlineCode",{parentName:"a"},"omp_set_num_threads"))," function in your program. The ",(0,a.kt)("inlineCode",{parentName:"p"},"#pragma")," tells the compiler that the rest of the line is a directive, and in this case it is omp parallel. ",(0,a.kt)("inlineCode",{parentName:"p"},"omp")," declares that it is for OpenMP and ",(0,a.kt)("inlineCode",{parentName:"p"},"parallel")," says the following code block (what is contained in { }) can be executed in parallel. Give it a try: ","`"," make hello && ./hello ","`"),(0,a.kt)("p",null,"If you run ",(0,a.kt)("inlineCode",{parentName:"p"},"./hello")," a couple of times, you should see that the numbers are not always in numerical order and will most likely vary across runs. This is because within the parallel region, OpenMP does the code in parallel and as a result does not enforce an ordering across all the threads. It is also vital to note that the variable ",(0,a.kt)("inlineCode",{parentName:"p"},"thread_ID")," is local to a specific thread and not shared across all threads. In general with OpenMP, variables declared inside the parallel block will be private to each thread, but variables declared outside will be global and accessible by all the threads."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"You will complete the next two exercises by modifying the functions inside ",(0,a.kt)("inlineCode",{parentName:"strong"},"omp_apps.c"),".")),(0,a.kt)("h2",{id:"exercise-2---vector-addition"},"Exercise 2 - Vector Addition"),(0,a.kt)("p",null,"Vector addition is a naturally parallel computation, so it makes for a good first exercise. The ",(0,a.kt)("inlineCode",{parentName:"p"},"v_add")," functions inside ",(0,a.kt)("inlineCode",{parentName:"p"},"omp_apps.c")," will return the array that is the cell-by-cell addition of its inputs x and y. A first attempt at this might look like:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"void v_add(double* x, double* y, double* z) {\n    #pragma omp parallel\n    {\n        for(int i=0; i<ARRAY_SIZE; i++)\n            z[i] = x[i] + y[i];\n    }\n}\n\n")),(0,a.kt)("p",null,"You can run this (",(0,a.kt)("inlineCode",{parentName:"p"},"make v_add")," followed by ",(0,a.kt)("inlineCode",{parentName:"p"},"./v_add"),") and the testing framework will vary the number of threads and time it. You will see that this actually seems to do worse as we increase the number of threads. The issue is that each thread is executing all of the code within the ",(0,a.kt)("inlineCode",{parentName:"p"},"omp parallel")," block, meaning if we have 8 threads, we will actually be adding the vectors 8 times. Rather than have each thread run the entire for loop, we need to split up the for loop across all the threads so each thread does only a portion of the work."),(0,a.kt)("p",null,"Your task is to optimize ",(0,a.kt)("inlineCode",{parentName:"p"},"v_add.c")," (speedup may plateau as the number of threads continues to increase). To aid you in this process, two useful OpenMP functions are:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"int omp_get_num_threads();")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"int omp_get_thread_num();"))),(0,a.kt)("p",null,"The function ",(0,a.kt)("inlineCode",{parentName:"p"},"omp_get_num_threads()")," will return how many threads there are in a ",(0,a.kt)("inlineCode",{parentName:"p"},"omp parallel")," block, and ",(0,a.kt)("inlineCode",{parentName:"p"},"omp_get_thread_num()")," will return the thread ID."),(0,a.kt)("p",null,"Divide up the work for each thread through two different methods (write different code for each of these methods):"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"First task, ",(0,a.kt)("strong",{parentName:"li"},"slicing"),": have each thread handle adjacent sums: i.e. Thread 0 will add the elements at indices ",(0,a.kt)("inlineCode",{parentName:"li"},"i")," such that ",(0,a.kt)("inlineCode",{parentName:"li"},"i % omp_get_num_threads()")," is ",(0,a.kt)("inlineCode",{parentName:"li"},"0"),", Thread 1 will add the elements where ",(0,a.kt)("inlineCode",{parentName:"li"},"i % omp_get_num_threads()")," is ",(0,a.kt)("inlineCode",{parentName:"li"},"1"),", etc."),(0,a.kt)("li",{parentName:"ol"},"Second task, ",(0,a.kt)("strong",{parentName:"li"},"chunking"),": if there are N threads, break the vectors into N contiguous chunks, and have each thread only add that chunk (like the figure above).")),(0,a.kt)("p",null,"Hints:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Use the two functions we listed above somehow in the for loop to choose which elements each thread handles."),(0,a.kt)("li",{parentName:"ul"},"You may need a special case to prevent going out of bounds for ",(0,a.kt)("inlineCode",{parentName:"li"},"v_add_optimized_chunks"),". Don\u2019t be afraid to write one."),(0,a.kt)("li",{parentName:"ul"},"Thinking about false sharing\u2013read more ",(0,a.kt)("a",{parentName:"li",href:"https://software.intel.com/en-us/articles/avoiding-and-identifying-false-sharing-among-threads"},"here")," and ",(0,a.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/False_sharing"},"here"),".")),(0,a.kt)("p",null,"For this exercise, we are asking you to manually split the work amongst threads since this is a common pattern used in software optimization. The designers of OpenMP actually made the ",(0,a.kt)("inlineCode",{parentName:"p"},"#pragma omp for")," directive to automatically split up independent work. Here is the function rewritten using it. ",(0,a.kt)("strong",{parentName:"p"},"You may NOT use this directive in your solution to this exercise"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"void v_add(double* x, double* y, double* z) {\n    #pragma omp parallel for \n    for(int i=0; i<ARRAY_SIZE; i++)\n        z[i] = x[i] + y[i];\n}\n\n")),(0,a.kt)("p",null,"Test the performance of your code with ",(0,a.kt)("inlineCode",{parentName:"p"},"make v_add && ./v_add")),(0,a.kt)("h2",{id:"exercise-3---dot-product"},"Exercise 3 - Dot Product"),(0,a.kt)("p",null,"The next task is to compute the dot product of two vectors. At first glance, implementing this might seem not too different from ",(0,a.kt)("inlineCode",{parentName:"p"},"v_add"),", but the challenge is how to sum up all of the products into the same variable (reduction). A sloppy handling of reduction may lead to ",(0,a.kt)("strong",{parentName:"p"},"data races"),": all the threads are trying to read and write to the same address simultaneously. One solution is to use a ",(0,a.kt)("strong",{parentName:"p"},"critical section"),". The code in a critical section can only be executed by a single thread at any given time. Thus, having a critical section naturally prevents multiple threads from reading and writing to the same data, a problem that would otherwise lead to data races. One way to avoid data races is to use the ",(0,a.kt)("inlineCode",{parentName:"p"},"critical")," primitive provided by OpenMP. An implementation, ",(0,a.kt)("inlineCode",{parentName:"p"},"dotp_naive")," in ",(0,a.kt)("inlineCode",{parentName:"p"},"omp_apps.c"),", protects the sum with a critical section."),(0,a.kt)("p",null,"Try out the code (",(0,a.kt)("inlineCode",{parentName:"p"},"make dotp &&./dotp"),"). Notice how the performance gets much worse as the number of threads goes up? By putting all of the work of reduction in a critical section, we have flattened the parallelism and made it so only one thread can do useful work at a time (not exactly the idea behind thread-level parallelism). This contention is problematic; each thread is constantly fighting for the critical section and only one is making any progress at any given time. As the number of threads goes up, so does the contention, and the performance pays the price. Can we reduce the number of times that each thread needs to use a critical section?"),(0,a.kt)("p",null,"Tasks:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Fix this performance problem ",(0,a.kt)("strong",{parentName:"li"},"without")," using the ",(0,a.kt)("inlineCode",{parentName:"li"},"reduction")," keyword in ",(0,a.kt)("inlineCode",{parentName:"li"},"dotp_manual_optimized"),". Remember that we want to reduce the number of times each thread enters the ",(0,a.kt)("inlineCode",{parentName:"li"},"critical")," section."),(0,a.kt)("li",{parentName:"ul"},"Next, fix this problem using OpenMP\u2019s built-in ",(0,a.kt)("inlineCode",{parentName:"li"},"reduction")," keyword in ",(0,a.kt)("inlineCode",{parentName:"li"},"dotp_reduction_optimized"),".(Note that your code should no longer contain ",(0,a.kt)("inlineCode",{parentName:"li"},"#pragma omp critical"),".)")),(0,a.kt)("h2",{id:"part-2-intro-to-multi-processing-programming"},"Part 2: Intro to multi-processing programming"),(0,a.kt)("p",null,"OpenMP is a convenient way to do multi-threading computation. Another common task level parallelism approach is multiprocessing. A thread is a single execution sequence that can be managed independently by the operating system. A process is an instance of a computer program that is being executed. It consists of an address space and one or more threads of control. It is the main abstraction for protection provided by the operating system kernel."),(0,a.kt)("p",null,"The key differences between multi-threading and multiprocessing is that in multi-threading, threads share the same address space, whereas in multiprocessing, each process has its own address space. Performance wise, this difference leads to two observations:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Threads have lower overhead (low memory and other resource footprint), and the cost of communication between threads is low as threads can only read/write to memory addresses in the same address space."),(0,a.kt)("li",{parentName:"ol"},"Sharing memory means we have to be careful about concurrency issues: when multiple threads can read/write to the same memory address, it can be hard to reason about correctness.")),(0,a.kt)("p",null,(0,a.kt)("img",{src:n(92721).Z,width:"1650",height:"1275"})," ",(0,a.kt)("img",{src:n(22836).Z,width:"1650",height:"1275"})),(0,a.kt)("p",null,"(credit to ",(0,a.kt)("a",{parentName:"p",href:"https://drawings.jvns.ca/"},"Julia Evans"),")"),(0,a.kt)("h3",{id:"background---http-web-server-and-multi-processing"},"Background - Http Web Server and Multi-processing"),(0,a.kt)("p",null,"In the second part of this lab, we will have a very basic but fun practice on writing multi-processing programs."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"fork")," syscall is used to create a new process by duplicating the calling process. If everything works fine, calling ",(0,a.kt)("inlineCode",{parentName:"p"},"fork")," should return the process ID of the child process being created to the calling process, and 0 to the newly created process (which is usually refered to as the child process). A negative value is returned if the creation of a child process failed. ",(0,a.kt)("a",{parentName:"p",href:"http://man7.org/linux/man-pages/man2/fork.2.html"},"Read more"),"."),(0,a.kt)("p",null,"For example, the following code:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'#include <stdio.h>\n#include <sys/types.h>\n#include <unistd.h>\nint main () {\n  pid_t child_pid;\n  printf("Main process id = %d (parent PID = %d)\\n",\n(int) getpid(), (int)  getppid());\n  child_pid = fork();\n  if (child_pid != 0)\nprintf("Parent: child\'s process id = %d\\n", child_pid);\n  else\nprintf("Child:  my process id = %d\\n", (int) getpid()); \n  return 0;\n}\n\n')),(0,a.kt)("p",null,"may output:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Main process id = 9075 (parent PID = 32146)\nParent: child's process id = 9076\nChild:  my process id = 9076\n\n")),(0,a.kt)("p",null,"Calling ",(0,a.kt)("inlineCode",{parentName:"p"},"fork()")," creates a child process. ",(0,a.kt)("strong",{parentName:"p"},"It might come as a surprise to you that this function returns to both the parent and the child process - the parent process gets the process ID of the child process being created, while 0 is returned to the child process. Thus, the parent and child processes diverge at the ",(0,a.kt)("inlineCode",{parentName:"strong"},"if")," block.")),(0,a.kt)("p",null,"Make sure you understand this code snippet before proceeding."),(0,a.kt)("p",null,"The program that we want you to parallelize is a basic HTTP web server. A web server creates a listening socket and binds it to a port, then waits for a client to connect to the port. Once a connection reqeust reaches, the server obtains a new connection socket, reads in and parses the HTTP request, then responds to the request by serving the requested file. For simplicity, the server program that we will be working with only reponds to \u201cGET\u201d requests."),(0,a.kt)("p",null,"A serial version is already implemented for you. To start, run ",(0,a.kt)("inlineCode",{parentName:"p"},"make server_basic && ./server_basic")," from the command line. This server program will run locally using ",(0,a.kt)("inlineCode",{parentName:"p"},"lab10/files/")," as the serve file directory, and listen to port 8000 by default (this can be changed using a command line argument, for example, ",(0,a.kt)("inlineCode",{parentName:"p"},"./server_basic --port 8080"),"). There are two ways to make a request: either open a browser and navigate to ",(0,a.kt)("inlineCode",{parentName:"p"},"localhost:8000/[request filename]")," or use the curl program: run ",(0,a.kt)("inlineCode",{parentName:"p"},"curl localhost:8000/[request filename]")," from the command line. (Type ",(0,a.kt)("inlineCode",{parentName:"p"},"man curl")," for more usage of ",(0,a.kt)("inlineCode",{parentName:"p"},"curl"),".)"),(0,a.kt)("p",null,"For our purpose here, the details of the server implementation can largely be ignored, but the function ",(0,a.kt)("inlineCode",{parentName:"p"},"server_forever")," defined in ",(0,a.kt)("inlineCode",{parentName:"p"},"server_utils.c")," needs your optimization. In this current implementation, the server program operates on a single process. Once the main process gets a request, it will work on serving the request before coming back to greet the next request. Therefore, if serving one request takes more than a blink \u2013 best luck on clients who need to be served later."),(0,a.kt)("p",null,"If the requested filename refers to a directory, the server first looks for the presence of an ",(0,a.kt)("strong",{parentName:"p"},"index.html")," file. If that is found, that webpage will be served. Otherwise it will present links to each file under the requested directory."),(0,a.kt)("p",null,"This server also offers two twists:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"If the request is ",(0,a.kt)("inlineCode",{parentName:"li"},"localhost:8000/report"),", it will run the ",(0,a.kt)("inlineCode",{parentName:"li"},"dotp")," program and serve the result in text. The ",(0,a.kt)("inlineCode",{parentName:"li"},"ARRAY_SIZE")," parameter has a default value set in ",(0,a.kt)("inlineCode",{parentName:"li"},"omp_apps.h"),", but you can change it from the command line: ",(0,a.kt)("inlineCode",{parentName:"li"},"./server_basic --dotp-size 10000000"),"."),(0,a.kt)("li",{parentName:"ul"},"It implements the routing feature. If the request is ",(0,a.kt)("inlineCode",{parentName:"li"},"/filter/[filename].bmp"),", it will run a very simple image processing program (specifially, the ",(0,a.kt)("a",{parentName:"li",href:"https://homepages.inf.ed.ac.uk/rbf/HIPR2/sobel.htm"},"sobel edge detector")," on the requested image, and return a html web page that display the original image and the filtered image together. A few sample bmp images are provided under files directory. For example, navigating to ",(0,a.kt)("inlineCode",{parentName:"li"},"localhost:8000/girl.bmp")," should get the original picture, but if you navigate to ",(0,a.kt)("inlineCode",{parentName:"li"},"localhost:8000/filter/girl.bmp"),", your browser should render the following:")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"sample",src:n(36554).Z,width:"529",height:"889"})),(0,a.kt)("h4",{id:"notes"},"Notes"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"If you do this exercise on a hive machine using ssh and want to make client requests using your local browser (sadly we can\u2019t just go to Soda Hall these days\u2026), say you ",(0,a.kt)("inlineCode",{parentName:"li"},"ssh")," to ",(0,a.kt)("inlineCode",{parentName:"li"},"[login]@hive9.cs.berkeley.edu")," and start the server using default setup (server listening to 127.0.0.1:8000), you can reqeust from browser using url ",(0,a.kt)("inlineCode",{parentName:"li"},"http://hive9.cs.berkeley.edu:8000/"),". And, of course, you can also use ",(0,a.kt)("inlineCode",{parentName:"li"},"curl")," from the command line."),(0,a.kt)("li",{parentName:"ul"},"The bmp library we use here is very basic. It lacks many relatively complicated padding features necessary to work with images of any sizes. In fact, the filter algorithm will only work nicely on bmp images that have dimensions of powers of 2. Don\u2019t be too surprised at seeing funky results if you try other image sources.")),(0,a.kt)("p",null,"Optional:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The sobel edge detector is implemented for you. Can you optimize it using OpenMP?")),(0,a.kt)("p",null,"(Feel free to implement other image processing algorithms and play with the server any way you like.)"),(0,a.kt)("p",null,"To simulate a costly computation, the request handler is made to wait 5 seconds after it finishes serving the requested file. You can easily observe this inefficiency by making two consecutive requests \u2013 the second one will halt for a while before getting a response."),(0,a.kt)("p",null,"Can we improve the server with some parallelism?"),(0,a.kt)("h3",{id:"exercise"},"Exercise:"),(0,a.kt)("p",null,"Instead of serving a request with the main process running the server program, always fork a new child process to do that and let the parent process continue to greet new requests. The ",(0,a.kt)("inlineCode",{parentName:"p"},"fork")," mechanics demonstrated in the sample code above should be adequate to help you finish the implementation."),(0,a.kt)("p",null,"The created child process has its own set of system resources (address space, registers, a stack, open handles to system objects, security context, etc). You need to explicitly clean up and recycle upon finishing the computation task. Take a look at ",(0,a.kt)("a",{parentName:"p",href:"https://linux.die.net/man/3/exit"},"the ",(0,a.kt)("inlineCode",{parentName:"a"},"exit")," syscall"),"."),(0,a.kt)("p",null,"To test your optimization, run ",(0,a.kt)("inlineCode",{parentName:"p"},"make server_process && ./server_process"),", then make two consecutive requests to any file, verify that the second request is immediately served. We provide a simple timer script, ",(0,a.kt)("inlineCode",{parentName:"p"},"timer.sh"),", to automate this process."),(0,a.kt)("p",null,"Run the compiled and linked server binary (either ",(0,a.kt)("inlineCode",{parentName:"p"},"server_basic")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"server_process")," in one terminal. Then, ",(0,a.kt)("strong",{parentName:"p"},"on another terminal")," (",(0,a.kt)("inlineCode",{parentName:"p"},"ssh")," to hive as needed), run ",(0,a.kt)("inlineCode",{parentName:"p"},"./timer.sh"),". The script would report the amount of time it takes to serve the requested files."),(0,a.kt)("p",null,"The performance gain of the forking implementation should be very impressive."),(0,a.kt)("p",null,"Note: Forked child processes are not guaranteed to be killed when you kill the parent process from command line by hitting Ctrl+C. This may lead to a side effect that the default port 8000 is occupied and you won\u2019t be able to restart your server program listening to the same port. The way to do it properly is out of scope for the purpose of 61C, so we have implemented this for you in the starter code. (You will learn to resolve issues like such in CS162, but you\u2019re encouraged to figure out what the starter code actually does on your own). We provide a work around here: If a port you attempt to use is occupied by a zombie process, you can kill it using command ",(0,a.kt)("inlineCode",{parentName:"p"},"fuser -k [port#]/tcp"),"."),(0,a.kt)("p",null,"Another work around is to use a different port number by passing in a command line argument, for example, ",(0,a.kt)("inlineCode",{parentName:"p"},"./server_process --port 8080"),"."),(0,a.kt)("p",null,"(FYI: forking a process to respond to a request is probably not the best parallelism approach \u2013 the modern solution is to use a thread pool where the overhead is much lower and you have convenient control over server load.)"),(0,a.kt)("h2",{id:"speedup-requirements"},"Speedup Requirements"),(0,a.kt)("p",null,"The lab autograder tests are slightly modified versions of the tests you have locally. In addition to correctness, it looks for some basic improvements in performance. In particular:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A ",(0,a.kt)("strong",{parentName:"li"},"2x")," speedup from the naive benchmark to the fastest adjacent/chunks runtime (out of all different number of threads) for ",(0,a.kt)("inlineCode",{parentName:"li"},"v_add"),"."),(0,a.kt)("li",{parentName:"ul"},"A ",(0,a.kt)("strong",{parentName:"li"},"9x")," speedup from the naive benchmark to the fastest manual/reduction runtime (out of all different number of threads) for ",(0,a.kt)("inlineCode",{parentName:"li"},"dotp"),"."),(0,a.kt)("li",{parentName:"ul"},"You should make sure your server responds within ",(0,a.kt)("strong",{parentName:"li"},"100ms")," for 3 consecutive requests (on the Hive), although the autograder has a more lenient requirement due to its limited CPU power.")),(0,a.kt)("h2",{id:"checkoff"},"Checkoff"),(0,a.kt)("p",null,"Please submit your code to the ",(0,a.kt)("strong",{parentName:"p"},"Lab Autograder")," assignment. Below are some questions to consider:"),(0,a.kt)("h3",{id:"part1"},"Part1"),(0,a.kt)("h4",{id:"exercise-2"},"Exercise 2:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Which version of your code runs faster, chunks or adjacent? What do you think the reason for this is? Explain to the person checking you off.")),(0,a.kt)("h4",{id:"exercise-3"},"Exercise 3:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Explain the difference in performance as the number of threads changes.")),(0,a.kt)("h3",{id:"part2"},"Part2"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"We see the speedup in the time taken to send consecutive requests. However, does our forking scheme increase the actual computation time?")))}u.isMDXComponent=!0},92721:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/process-c30b01333a7a147bb5d8e3e62f3610dd.png"},36554:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/sample_output-33cffb79344e28bd8f8ba7a44b52a09e.jpg"},22836:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/threads-41b75746e19ffbc4854e7824cc13d5f4.png"}}]);